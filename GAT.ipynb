{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "From Networks to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T04:02:33.377469Z",
     "start_time": "2019-01-20T04:02:33.370225Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Switching to deep learning**\n",
    "- The growth lab https://growthlab.cid.harvard.edu/\n",
    "\n",
    "- When we talk about the reality with mathematics or physics models, we are trying to reduce the complexity. Thus, we lose our understanding about the rich resolution.\n",
    "- When there are clear-cut patterns, building up models could be a good idea. Or else, it may fail easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Graph Attention Networks**\n",
    "\n",
    "- pygat https://github.com/Diego999/pyGAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Graph Convolutional Network**\n",
    "\n",
    "- pygcn https://github.com/tkipf/pygcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "<img src = 'cnn.png' width = '400'>\n",
    "http://petar-v.com/GAT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T06:06:56.893240Z",
     "start_time": "2019-01-20T06:06:56.889300Z"
    }
   },
   "source": [
    "<img src = 'cnn2.png' width = '700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepwalk\n",
    " \n",
    "- uses local information obtained from **truncated random walks** \n",
    "    - to learn latent representations by \n",
    "- treating walks as the equivalent of sentences.\n",
    "- Using word2vec method\n",
    "\n",
    "DeepWalk: Online Learning of Social Representations. KDD2014 http://www.perozzi.net/projects/deepwalk/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T06:29:33.927252Z",
     "start_time": "2019-01-20T06:29:33.922784Z"
    }
   },
   "source": [
    "# Node2vec\n",
    "\n",
    "- it provides a way of balancing the exploration-exploitation tradeoff that in turn \n",
    "- leads to representations obeying a spectrum of equivalences from homophily to structural equivalence.\n",
    "\n",
    "<img src = 'walk.png' width = '300'>\n",
    "\n",
    "http://snap.stanford.edu/node2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Network\n",
    "\n",
    "- $X$ is a matrix of node feature vectors.\n",
    "- $A$ is an adjacent matrix.\n",
    "\n",
    "$y  \\sim f(X, A)$\n",
    "\n",
    "Consider a multi-layer Graph Convolutional Network (GCN) with the following layer-wise propagation rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T06:10:29.314898Z",
     "start_time": "2019-01-20T06:10:29.310786Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'gcn.png' width = '700'>\n",
    "\n",
    "https://github.com/tkipf/pygcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Cora dataset**\n",
    "- www.research.whizbang.com/data\n",
    "- consists of Machine Learning papers. These papers are classified into one of the following seven classes:\n",
    "Case_Based\n",
    "Genetic_Algorithms\n",
    "Neural_Networks\n",
    "Probabilistic_Methods\n",
    "Reinforcement_Learning\n",
    "Rule_Learning\n",
    "Theory\n",
    "- citation network \n",
    "- There are 2708 papers in the whole corpus. \n",
    "- a vocabulary of size 1433 unique words. \n",
    "    - All words with document frequency less than 10 were removed.\n",
    "- The .content file contains descriptions of the papers in the following format:\n",
    "\n",
    "\t\t<paper_id> <word_attributes>+ <class_label>\n",
    "\n",
    "- The .cites file contains the citation graph of the corpus. Each line describes a link in the following format:\n",
    "\n",
    "\t\t<ID of cited paper> <ID of citing paper>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T07:13:51.179694Z",
     "start_time": "2019-01-20T07:13:51.175773Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'gcn2.png' width = '700'>\n",
    "https://arxiv.org/pdf/1609.02907.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:05:19.031413Z",
     "start_time": "2019-01-20T09:05:18.819578Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#import pygcn\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:05:19.691079Z",
     "start_time": "2019-01-20T09:05:19.667612Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:21:26.729681Z",
     "start_time": "2019-01-20T09:21:26.725935Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:05:21.940373Z",
     "start_time": "2019-01-20T09:05:21.922402Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda = True\n",
    "        self.fastmode = False\n",
    "        self.seed = 42\n",
    "        self.epochs = 200\n",
    "        self.lr = 0.01\n",
    "        self.weight_decay = 5e-4\n",
    "        self.hidden = 16\n",
    "        self.dropout = 0.5\n",
    "        self.cuda = False\n",
    "        \n",
    "args = Args()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:05:20.381723Z",
     "start_time": "2019-01-20T09:05:20.267862Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT.ipynb  cnn.png    cnn2.png   gcn.png    gcn2.png   \u001b[34mpygat\u001b[m\u001b[m/     walk.png\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:42:24.741727Z",
     "start_time": "2019-01-20T09:42:18.888777Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "path = '/Users/datalab/github/deeplearning-winter2019/pygcn-master/data/cora/'\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:46:24.515818Z",
     "start_time": "2019-01-20T09:46:24.510897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2708, 2708]),\n",
       " torch.Size([2708, 1433]),\n",
       " torch.Size([2708]),\n",
       " torch.Size([140]),\n",
       " torch.Size([300]),\n",
       " torch.Size([1000]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape, features.shape, labels.shape, idx_train.shape, idx_val.shape, idx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:46:56.293259Z",
     "start_time": "2019-01-20T09:46:56.286943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0.0: 1413, 0.05: 20}),\n",
       " Counter({0: 180, 1: 298, 2: 418, 3: 426, 4: 818, 5: 351, 6: 217}))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(features[0].data.numpy()), Counter(labels.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:50:21.569108Z",
     "start_time": "2019-01-20T09:50:21.340192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = [3, 3])\n",
    "plt.imshow(adj.to_dense())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:11:25.853748Z",
     "start_time": "2019-01-20T09:11:25.848721Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1433, 16, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[1], args.hidden, labels.max().item() + 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:08:50.142069Z",
     "start_time": "2019-01-20T09:08:50.126795Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T10:27:05.464279Z",
     "start_time": "2019-01-20T10:27:05.409794Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T10:27:14.203416Z",
     "start_time": "2019-01-20T10:27:08.429378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "path = '/Users/datalab/github/deeplearning-winter2019/pygcn-master/data/cora/'\n",
    "dataset = 'cora'\n",
    "\"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                    dtype=np.dtype(str))\n",
    "features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "# build graph\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "idx_map = {j: i for i, j in enumerate(idx)}\n",
    "edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                dtype=np.int32)\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                 dtype=np.int32).reshape(edges_unordered.shape)\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(labels.shape[0], labels.shape[0]),\n",
    "                    dtype=np.float32)\n",
    "\n",
    "# build symmetric adjacency matrix\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "features = normalize(features)\n",
    "adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "idx_train = range(140)\n",
    "idx_val = range(200, 500)\n",
    "idx_test = range(500, 1500)\n",
    "\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "labels = torch.LongTensor(np.where(labels)[1])\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T10:27:20.781757Z",
     "start_time": "2019-01-20T10:27:20.777277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 163,  402],\n",
       "       [ 163,  659],\n",
       "       [ 163, 1696],\n",
       "       ...,\n",
       "       [1887, 2258],\n",
       "       [1902, 1887],\n",
       "       [ 837, 1686]], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T10:34:47.017737Z",
     "start_time": "2019-01-20T10:34:46.995300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 0. 0.]] \n",
      " 1st\n",
      "[[0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]] \n",
      " 2nd\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]] \n",
      " 3rd\n"
     ]
    }
   ],
   "source": [
    "edges = np.array([[ 1,  0],\n",
    "       [ 0,  2],\n",
    "       [ 1, 2]])\n",
    "\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(3, 3),\n",
    "                    dtype=np.float32)\n",
    "print(adj.toarray(), '\\n 1st')\n",
    "\n",
    "# build symmetric adjacency matrix\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "print(adj.toarray(), '\\n 2nd')\n",
    "\n",
    "adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "print(adj.toarray(), '\\n 3rd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T10:30:02.289298Z",
     "start_time": "2019-01-20T10:30:02.284111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:55:08.278960Z",
     "start_time": "2019-01-20T09:55:08.272988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000e+00, -8.5899e+09, -5.5325e+16, -3.6902e+19],\n",
       "        [ 5.9704e-33,  1.4013e-45,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7551e-40]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "Parameter(torch.FloatTensor(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:09:06.062683Z",
     "start_time": "2019-01-20T09:09:01.387934Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9452 acc_train: 0.1143 loss_val: 1.9303 acc_val: 0.1067 time: 0.0365s\n",
      "Epoch: 0002 loss_train: 1.9365 acc_train: 0.1214 loss_val: 1.9173 acc_val: 0.1033 time: 0.0243s\n",
      "Epoch: 0003 loss_train: 1.9299 acc_train: 0.1429 loss_val: 1.9047 acc_val: 0.3467 time: 0.0222s\n",
      "Epoch: 0004 loss_train: 1.9153 acc_train: 0.2643 loss_val: 1.8925 acc_val: 0.3500 time: 0.0218s\n",
      "Epoch: 0005 loss_train: 1.9009 acc_train: 0.2857 loss_val: 1.8807 acc_val: 0.3500 time: 0.0199s\n",
      "Epoch: 0006 loss_train: 1.8823 acc_train: 0.3000 loss_val: 1.8693 acc_val: 0.3500 time: 0.0198s\n",
      "Epoch: 0007 loss_train: 1.8764 acc_train: 0.2929 loss_val: 1.8585 acc_val: 0.3500 time: 0.0198s\n",
      "Epoch: 0008 loss_train: 1.8630 acc_train: 0.2929 loss_val: 1.8482 acc_val: 0.3500 time: 0.0220s\n",
      "Epoch: 0009 loss_train: 1.8544 acc_train: 0.2929 loss_val: 1.8382 acc_val: 0.3500 time: 0.0219s\n",
      "Epoch: 0010 loss_train: 1.8489 acc_train: 0.2929 loss_val: 1.8286 acc_val: 0.3500 time: 0.0190s\n",
      "Epoch: 0011 loss_train: 1.8329 acc_train: 0.2929 loss_val: 1.8190 acc_val: 0.3500 time: 0.0220s\n",
      "Epoch: 0012 loss_train: 1.8221 acc_train: 0.2929 loss_val: 1.8099 acc_val: 0.3500 time: 0.0239s\n",
      "Epoch: 0013 loss_train: 1.8178 acc_train: 0.2929 loss_val: 1.8012 acc_val: 0.3500 time: 0.0232s\n",
      "Epoch: 0014 loss_train: 1.7883 acc_train: 0.2929 loss_val: 1.7930 acc_val: 0.3500 time: 0.0226s\n",
      "Epoch: 0015 loss_train: 1.7903 acc_train: 0.2929 loss_val: 1.7852 acc_val: 0.3500 time: 0.0236s\n",
      "Epoch: 0016 loss_train: 1.7924 acc_train: 0.2857 loss_val: 1.7779 acc_val: 0.3500 time: 0.0220s\n",
      "Epoch: 0017 loss_train: 1.7664 acc_train: 0.3143 loss_val: 1.7711 acc_val: 0.3500 time: 0.0216s\n",
      "Epoch: 0018 loss_train: 1.7690 acc_train: 0.2929 loss_val: 1.7646 acc_val: 0.3500 time: 0.0215s\n",
      "Epoch: 0019 loss_train: 1.7442 acc_train: 0.3500 loss_val: 1.7584 acc_val: 0.3500 time: 0.0219s\n",
      "Epoch: 0020 loss_train: 1.7640 acc_train: 0.2929 loss_val: 1.7524 acc_val: 0.3500 time: 0.0217s\n",
      "Epoch: 0021 loss_train: 1.7517 acc_train: 0.3071 loss_val: 1.7466 acc_val: 0.3500 time: 0.0235s\n",
      "Epoch: 0022 loss_train: 1.7247 acc_train: 0.3214 loss_val: 1.7407 acc_val: 0.3500 time: 0.0228s\n",
      "Epoch: 0023 loss_train: 1.7307 acc_train: 0.3071 loss_val: 1.7349 acc_val: 0.3500 time: 0.0247s\n",
      "Epoch: 0024 loss_train: 1.7253 acc_train: 0.3214 loss_val: 1.7290 acc_val: 0.3500 time: 0.0222s\n",
      "Epoch: 0025 loss_train: 1.7085 acc_train: 0.3286 loss_val: 1.7228 acc_val: 0.3500 time: 0.0211s\n",
      "Epoch: 0026 loss_train: 1.7163 acc_train: 0.3143 loss_val: 1.7164 acc_val: 0.3500 time: 0.0223s\n",
      "Epoch: 0027 loss_train: 1.6951 acc_train: 0.3429 loss_val: 1.7097 acc_val: 0.3500 time: 0.0222s\n",
      "Epoch: 0028 loss_train: 1.6934 acc_train: 0.3000 loss_val: 1.7028 acc_val: 0.3500 time: 0.0224s\n",
      "Epoch: 0029 loss_train: 1.6702 acc_train: 0.3286 loss_val: 1.6958 acc_val: 0.3500 time: 0.0228s\n",
      "Epoch: 0030 loss_train: 1.6648 acc_train: 0.3286 loss_val: 1.6888 acc_val: 0.3500 time: 0.0262s\n",
      "Epoch: 0031 loss_train: 1.6632 acc_train: 0.3357 loss_val: 1.6814 acc_val: 0.3500 time: 0.0240s\n",
      "Epoch: 0032 loss_train: 1.6484 acc_train: 0.3286 loss_val: 1.6739 acc_val: 0.3500 time: 0.0252s\n",
      "Epoch: 0033 loss_train: 1.6435 acc_train: 0.3500 loss_val: 1.6661 acc_val: 0.3533 time: 0.0259s\n",
      "Epoch: 0034 loss_train: 1.6196 acc_train: 0.3214 loss_val: 1.6581 acc_val: 0.3600 time: 0.0274s\n",
      "Epoch: 0035 loss_train: 1.6098 acc_train: 0.3286 loss_val: 1.6500 acc_val: 0.3633 time: 0.0223s\n",
      "Epoch: 0036 loss_train: 1.6081 acc_train: 0.3714 loss_val: 1.6418 acc_val: 0.3633 time: 0.0197s\n",
      "Epoch: 0037 loss_train: 1.5767 acc_train: 0.3857 loss_val: 1.6334 acc_val: 0.3633 time: 0.0198s\n",
      "Epoch: 0038 loss_train: 1.5876 acc_train: 0.3786 loss_val: 1.6248 acc_val: 0.3633 time: 0.0227s\n",
      "Epoch: 0039 loss_train: 1.5519 acc_train: 0.4071 loss_val: 1.6158 acc_val: 0.3700 time: 0.0223s\n",
      "Epoch: 0040 loss_train: 1.5394 acc_train: 0.4214 loss_val: 1.6064 acc_val: 0.3733 time: 0.0221s\n",
      "Epoch: 0041 loss_train: 1.5589 acc_train: 0.4214 loss_val: 1.5966 acc_val: 0.3767 time: 0.0222s\n",
      "Epoch: 0042 loss_train: 1.5369 acc_train: 0.4071 loss_val: 1.5866 acc_val: 0.3900 time: 0.0221s\n",
      "Epoch: 0043 loss_train: 1.5283 acc_train: 0.3857 loss_val: 1.5764 acc_val: 0.3933 time: 0.0224s\n",
      "Epoch: 0044 loss_train: 1.4935 acc_train: 0.4286 loss_val: 1.5659 acc_val: 0.4033 time: 0.0192s\n",
      "Epoch: 0045 loss_train: 1.4761 acc_train: 0.4500 loss_val: 1.5554 acc_val: 0.4100 time: 0.0210s\n",
      "Epoch: 0046 loss_train: 1.4850 acc_train: 0.4429 loss_val: 1.5448 acc_val: 0.4100 time: 0.0214s\n",
      "Epoch: 0047 loss_train: 1.4738 acc_train: 0.4214 loss_val: 1.5336 acc_val: 0.4100 time: 0.0197s\n",
      "Epoch: 0048 loss_train: 1.4677 acc_train: 0.4143 loss_val: 1.5222 acc_val: 0.4167 time: 0.0218s\n",
      "Epoch: 0049 loss_train: 1.3986 acc_train: 0.4571 loss_val: 1.5104 acc_val: 0.4167 time: 0.0234s\n",
      "Epoch: 0050 loss_train: 1.4091 acc_train: 0.4643 loss_val: 1.4985 acc_val: 0.4233 time: 0.0201s\n",
      "Epoch: 0051 loss_train: 1.3882 acc_train: 0.4500 loss_val: 1.4866 acc_val: 0.4233 time: 0.0261s\n",
      "Epoch: 0052 loss_train: 1.3681 acc_train: 0.4500 loss_val: 1.4749 acc_val: 0.4300 time: 0.0264s\n",
      "Epoch: 0053 loss_train: 1.3805 acc_train: 0.4571 loss_val: 1.4632 acc_val: 0.4433 time: 0.0221s\n",
      "Epoch: 0054 loss_train: 1.3438 acc_train: 0.4643 loss_val: 1.4515 acc_val: 0.4533 time: 0.0233s\n",
      "Epoch: 0055 loss_train: 1.3394 acc_train: 0.4643 loss_val: 1.4397 acc_val: 0.4600 time: 0.0240s\n",
      "Epoch: 0056 loss_train: 1.3014 acc_train: 0.4500 loss_val: 1.4277 acc_val: 0.4667 time: 0.0232s\n",
      "Epoch: 0057 loss_train: 1.3206 acc_train: 0.4857 loss_val: 1.4160 acc_val: 0.4733 time: 0.0236s\n",
      "Epoch: 0058 loss_train: 1.2972 acc_train: 0.4786 loss_val: 1.4043 acc_val: 0.4733 time: 0.0253s\n",
      "Epoch: 0059 loss_train: 1.3009 acc_train: 0.5071 loss_val: 1.3927 acc_val: 0.4733 time: 0.0235s\n",
      "Epoch: 0060 loss_train: 1.2487 acc_train: 0.5214 loss_val: 1.3810 acc_val: 0.4767 time: 0.0224s\n",
      "Epoch: 0061 loss_train: 1.2668 acc_train: 0.5143 loss_val: 1.3692 acc_val: 0.4800 time: 0.0235s\n",
      "Epoch: 0062 loss_train: 1.2217 acc_train: 0.5071 loss_val: 1.3578 acc_val: 0.4833 time: 0.0228s\n",
      "Epoch: 0063 loss_train: 1.2229 acc_train: 0.5429 loss_val: 1.3465 acc_val: 0.5000 time: 0.0235s\n",
      "Epoch: 0064 loss_train: 1.2044 acc_train: 0.5071 loss_val: 1.3356 acc_val: 0.5167 time: 0.0227s\n",
      "Epoch: 0065 loss_train: 1.1928 acc_train: 0.5643 loss_val: 1.3245 acc_val: 0.5267 time: 0.0207s\n",
      "Epoch: 0066 loss_train: 1.1481 acc_train: 0.5786 loss_val: 1.3135 acc_val: 0.5333 time: 0.0243s\n",
      "Epoch: 0067 loss_train: 1.1656 acc_train: 0.5786 loss_val: 1.3026 acc_val: 0.5500 time: 0.0241s\n",
      "Epoch: 0068 loss_train: 1.1376 acc_train: 0.6143 loss_val: 1.2917 acc_val: 0.5600 time: 0.0244s\n",
      "Epoch: 0069 loss_train: 1.1281 acc_train: 0.6357 loss_val: 1.2809 acc_val: 0.5733 time: 0.0229s\n",
      "Epoch: 0070 loss_train: 1.1067 acc_train: 0.6786 loss_val: 1.2702 acc_val: 0.5900 time: 0.0252s\n",
      "Epoch: 0071 loss_train: 1.0963 acc_train: 0.6286 loss_val: 1.2596 acc_val: 0.6100 time: 0.0199s\n",
      "Epoch: 0072 loss_train: 1.1234 acc_train: 0.6286 loss_val: 1.2494 acc_val: 0.6200 time: 0.0230s\n",
      "Epoch: 0073 loss_train: 1.1163 acc_train: 0.6286 loss_val: 1.2397 acc_val: 0.6367 time: 0.0236s\n",
      "Epoch: 0074 loss_train: 1.0534 acc_train: 0.7286 loss_val: 1.2302 acc_val: 0.6433 time: 0.0228s\n",
      "Epoch: 0075 loss_train: 1.0734 acc_train: 0.7000 loss_val: 1.2206 acc_val: 0.6600 time: 0.0231s\n",
      "Epoch: 0076 loss_train: 1.0432 acc_train: 0.7286 loss_val: 1.2104 acc_val: 0.6733 time: 0.0244s\n",
      "Epoch: 0077 loss_train: 1.0332 acc_train: 0.7571 loss_val: 1.2000 acc_val: 0.6733 time: 0.0228s\n",
      "Epoch: 0078 loss_train: 1.0049 acc_train: 0.7357 loss_val: 1.1896 acc_val: 0.6767 time: 0.0218s\n",
      "Epoch: 0079 loss_train: 1.0048 acc_train: 0.7357 loss_val: 1.1793 acc_val: 0.6767 time: 0.0215s\n",
      "Epoch: 0080 loss_train: 0.9998 acc_train: 0.7143 loss_val: 1.1692 acc_val: 0.6867 time: 0.0221s\n",
      "Epoch: 0081 loss_train: 0.9828 acc_train: 0.7500 loss_val: 1.1590 acc_val: 0.6833 time: 0.0247s\n",
      "Epoch: 0082 loss_train: 0.9414 acc_train: 0.7857 loss_val: 1.1489 acc_val: 0.6967 time: 0.0236s\n",
      "Epoch: 0083 loss_train: 1.0121 acc_train: 0.7571 loss_val: 1.1390 acc_val: 0.7033 time: 0.0243s\n",
      "Epoch: 0084 loss_train: 0.9475 acc_train: 0.7643 loss_val: 1.1291 acc_val: 0.7067 time: 0.0216s\n",
      "Epoch: 0085 loss_train: 0.9455 acc_train: 0.7643 loss_val: 1.1193 acc_val: 0.7067 time: 0.0249s\n",
      "Epoch: 0086 loss_train: 0.9467 acc_train: 0.7571 loss_val: 1.1096 acc_val: 0.7167 time: 0.0218s\n",
      "Epoch: 0087 loss_train: 0.9639 acc_train: 0.7929 loss_val: 1.0993 acc_val: 0.7200 time: 0.0202s\n",
      "Epoch: 0088 loss_train: 0.9238 acc_train: 0.8000 loss_val: 1.0892 acc_val: 0.7200 time: 0.0232s\n",
      "Epoch: 0089 loss_train: 0.9028 acc_train: 0.8071 loss_val: 1.0793 acc_val: 0.7267 time: 0.0230s\n",
      "Epoch: 0090 loss_train: 0.8966 acc_train: 0.8000 loss_val: 1.0695 acc_val: 0.7333 time: 0.0206s\n",
      "Epoch: 0091 loss_train: 0.8404 acc_train: 0.8071 loss_val: 1.0600 acc_val: 0.7400 time: 0.0251s\n",
      "Epoch: 0092 loss_train: 0.8775 acc_train: 0.7857 loss_val: 1.0508 acc_val: 0.7467 time: 0.0200s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0093 loss_train: 0.8302 acc_train: 0.8357 loss_val: 1.0411 acc_val: 0.7533 time: 0.0233s\n",
      "Epoch: 0094 loss_train: 0.8731 acc_train: 0.8000 loss_val: 1.0311 acc_val: 0.7567 time: 0.0242s\n",
      "Epoch: 0095 loss_train: 0.8644 acc_train: 0.8214 loss_val: 1.0211 acc_val: 0.7600 time: 0.0249s\n",
      "Epoch: 0096 loss_train: 0.8513 acc_train: 0.8071 loss_val: 1.0110 acc_val: 0.7600 time: 0.0244s\n",
      "Epoch: 0097 loss_train: 0.8201 acc_train: 0.8429 loss_val: 1.0009 acc_val: 0.7567 time: 0.0217s\n",
      "Epoch: 0098 loss_train: 0.8056 acc_train: 0.8357 loss_val: 0.9914 acc_val: 0.7567 time: 0.0240s\n",
      "Epoch: 0099 loss_train: 0.7770 acc_train: 0.8643 loss_val: 0.9829 acc_val: 0.7600 time: 0.0230s\n",
      "Epoch: 0100 loss_train: 0.8201 acc_train: 0.8214 loss_val: 0.9747 acc_val: 0.7633 time: 0.0221s\n",
      "Epoch: 0101 loss_train: 0.8101 acc_train: 0.8000 loss_val: 0.9667 acc_val: 0.7733 time: 0.0235s\n",
      "Epoch: 0102 loss_train: 0.7146 acc_train: 0.8429 loss_val: 0.9591 acc_val: 0.7800 time: 0.0217s\n",
      "Epoch: 0103 loss_train: 0.7087 acc_train: 0.8714 loss_val: 0.9519 acc_val: 0.7800 time: 0.0224s\n",
      "Epoch: 0104 loss_train: 0.7328 acc_train: 0.8643 loss_val: 0.9451 acc_val: 0.7867 time: 0.0220s\n",
      "Epoch: 0105 loss_train: 0.7347 acc_train: 0.8286 loss_val: 0.9382 acc_val: 0.7967 time: 0.0207s\n",
      "Epoch: 0106 loss_train: 0.7103 acc_train: 0.8571 loss_val: 0.9311 acc_val: 0.7967 time: 0.0221s\n",
      "Epoch: 0107 loss_train: 0.6886 acc_train: 0.8571 loss_val: 0.9241 acc_val: 0.7967 time: 0.0239s\n",
      "Epoch: 0108 loss_train: 0.7363 acc_train: 0.8286 loss_val: 0.9175 acc_val: 0.7967 time: 0.0221s\n",
      "Epoch: 0109 loss_train: 0.7162 acc_train: 0.8857 loss_val: 0.9112 acc_val: 0.8000 time: 0.0246s\n",
      "Epoch: 0110 loss_train: 0.7284 acc_train: 0.8357 loss_val: 0.9049 acc_val: 0.8000 time: 0.0225s\n",
      "Epoch: 0111 loss_train: 0.6803 acc_train: 0.8857 loss_val: 0.8988 acc_val: 0.7967 time: 0.0242s\n",
      "Epoch: 0112 loss_train: 0.6924 acc_train: 0.9071 loss_val: 0.8926 acc_val: 0.7967 time: 0.0223s\n",
      "Epoch: 0113 loss_train: 0.7158 acc_train: 0.8357 loss_val: 0.8865 acc_val: 0.7967 time: 0.0254s\n",
      "Epoch: 0114 loss_train: 0.6908 acc_train: 0.8571 loss_val: 0.8809 acc_val: 0.8033 time: 0.0258s\n",
      "Epoch: 0115 loss_train: 0.6811 acc_train: 0.8857 loss_val: 0.8745 acc_val: 0.8100 time: 0.0231s\n",
      "Epoch: 0116 loss_train: 0.6991 acc_train: 0.8571 loss_val: 0.8696 acc_val: 0.8133 time: 0.0210s\n",
      "Epoch: 0117 loss_train: 0.6604 acc_train: 0.8857 loss_val: 0.8656 acc_val: 0.8067 time: 0.0220s\n",
      "Epoch: 0118 loss_train: 0.6412 acc_train: 0.9143 loss_val: 0.8620 acc_val: 0.8033 time: 0.0213s\n",
      "Epoch: 0119 loss_train: 0.6465 acc_train: 0.8857 loss_val: 0.8591 acc_val: 0.8033 time: 0.0196s\n",
      "Epoch: 0120 loss_train: 0.6257 acc_train: 0.9143 loss_val: 0.8553 acc_val: 0.8033 time: 0.0219s\n",
      "Epoch: 0121 loss_train: 0.6250 acc_train: 0.8357 loss_val: 0.8510 acc_val: 0.8033 time: 0.0214s\n",
      "Epoch: 0122 loss_train: 0.6076 acc_train: 0.8929 loss_val: 0.8464 acc_val: 0.8033 time: 0.0210s\n",
      "Epoch: 0123 loss_train: 0.5974 acc_train: 0.9286 loss_val: 0.8418 acc_val: 0.8033 time: 0.0204s\n",
      "Epoch: 0124 loss_train: 0.6219 acc_train: 0.8571 loss_val: 0.8381 acc_val: 0.8033 time: 0.0212s\n",
      "Epoch: 0125 loss_train: 0.6313 acc_train: 0.8857 loss_val: 0.8346 acc_val: 0.8033 time: 0.0300s\n",
      "Epoch: 0126 loss_train: 0.5715 acc_train: 0.8929 loss_val: 0.8309 acc_val: 0.8033 time: 0.0223s\n",
      "Epoch: 0127 loss_train: 0.6059 acc_train: 0.8857 loss_val: 0.8275 acc_val: 0.8033 time: 0.0201s\n",
      "Epoch: 0128 loss_train: 0.6227 acc_train: 0.8786 loss_val: 0.8234 acc_val: 0.8033 time: 0.0249s\n",
      "Epoch: 0129 loss_train: 0.5721 acc_train: 0.9071 loss_val: 0.8197 acc_val: 0.8033 time: 0.0219s\n",
      "Epoch: 0130 loss_train: 0.5920 acc_train: 0.8929 loss_val: 0.8166 acc_val: 0.8033 time: 0.0223s\n",
      "Epoch: 0131 loss_train: 0.5877 acc_train: 0.9071 loss_val: 0.8139 acc_val: 0.8033 time: 0.0223s\n",
      "Epoch: 0132 loss_train: 0.6060 acc_train: 0.8643 loss_val: 0.8116 acc_val: 0.8033 time: 0.0216s\n",
      "Epoch: 0133 loss_train: 0.5753 acc_train: 0.8786 loss_val: 0.8100 acc_val: 0.8067 time: 0.0178s\n",
      "Epoch: 0134 loss_train: 0.5223 acc_train: 0.9143 loss_val: 0.8082 acc_val: 0.8067 time: 0.0176s\n",
      "Epoch: 0135 loss_train: 0.5535 acc_train: 0.9000 loss_val: 0.8070 acc_val: 0.8033 time: 0.0195s\n",
      "Epoch: 0136 loss_train: 0.5837 acc_train: 0.9143 loss_val: 0.8046 acc_val: 0.8033 time: 0.0198s\n",
      "Epoch: 0137 loss_train: 0.5792 acc_train: 0.9286 loss_val: 0.8018 acc_val: 0.8033 time: 0.0209s\n",
      "Epoch: 0138 loss_train: 0.5472 acc_train: 0.9357 loss_val: 0.7994 acc_val: 0.8033 time: 0.0207s\n",
      "Epoch: 0139 loss_train: 0.5724 acc_train: 0.8929 loss_val: 0.7957 acc_val: 0.8033 time: 0.0215s\n",
      "Epoch: 0140 loss_train: 0.5592 acc_train: 0.9286 loss_val: 0.7925 acc_val: 0.8033 time: 0.0227s\n",
      "Epoch: 0141 loss_train: 0.5723 acc_train: 0.8786 loss_val: 0.7894 acc_val: 0.8067 time: 0.0190s\n",
      "Epoch: 0142 loss_train: 0.5395 acc_train: 0.9000 loss_val: 0.7865 acc_val: 0.8100 time: 0.0196s\n",
      "Epoch: 0143 loss_train: 0.5578 acc_train: 0.8857 loss_val: 0.7834 acc_val: 0.8167 time: 0.0217s\n",
      "Epoch: 0144 loss_train: 0.5345 acc_train: 0.8929 loss_val: 0.7806 acc_val: 0.8167 time: 0.0227s\n",
      "Epoch: 0145 loss_train: 0.5605 acc_train: 0.8786 loss_val: 0.7778 acc_val: 0.8100 time: 0.0231s\n",
      "Epoch: 0146 loss_train: 0.5287 acc_train: 0.8929 loss_val: 0.7759 acc_val: 0.8067 time: 0.0229s\n",
      "Epoch: 0147 loss_train: 0.5201 acc_train: 0.9214 loss_val: 0.7744 acc_val: 0.8033 time: 0.0220s\n",
      "Epoch: 0148 loss_train: 0.5304 acc_train: 0.8929 loss_val: 0.7729 acc_val: 0.8033 time: 0.0228s\n",
      "Epoch: 0149 loss_train: 0.5166 acc_train: 0.9286 loss_val: 0.7710 acc_val: 0.8067 time: 0.0226s\n",
      "Epoch: 0150 loss_train: 0.4981 acc_train: 0.9214 loss_val: 0.7687 acc_val: 0.8067 time: 0.0249s\n",
      "Epoch: 0151 loss_train: 0.5322 acc_train: 0.9214 loss_val: 0.7665 acc_val: 0.8033 time: 0.0255s\n",
      "Epoch: 0152 loss_train: 0.5165 acc_train: 0.8929 loss_val: 0.7642 acc_val: 0.8000 time: 0.0229s\n",
      "Epoch: 0153 loss_train: 0.5025 acc_train: 0.9000 loss_val: 0.7617 acc_val: 0.8033 time: 0.0235s\n",
      "Epoch: 0154 loss_train: 0.4937 acc_train: 0.9286 loss_val: 0.7588 acc_val: 0.8033 time: 0.0237s\n",
      "Epoch: 0155 loss_train: 0.4806 acc_train: 0.9286 loss_val: 0.7561 acc_val: 0.8067 time: 0.0251s\n",
      "Epoch: 0156 loss_train: 0.4994 acc_train: 0.8929 loss_val: 0.7543 acc_val: 0.8033 time: 0.0245s\n",
      "Epoch: 0157 loss_train: 0.5232 acc_train: 0.9143 loss_val: 0.7525 acc_val: 0.8033 time: 0.0228s\n",
      "Epoch: 0158 loss_train: 0.5119 acc_train: 0.9429 loss_val: 0.7496 acc_val: 0.8100 time: 0.0229s\n",
      "Epoch: 0159 loss_train: 0.5044 acc_train: 0.9000 loss_val: 0.7466 acc_val: 0.8133 time: 0.0239s\n",
      "Epoch: 0160 loss_train: 0.4839 acc_train: 0.9000 loss_val: 0.7438 acc_val: 0.8167 time: 0.0234s\n",
      "Epoch: 0161 loss_train: 0.4711 acc_train: 0.9143 loss_val: 0.7404 acc_val: 0.8167 time: 0.0239s\n",
      "Epoch: 0162 loss_train: 0.4896 acc_train: 0.9357 loss_val: 0.7367 acc_val: 0.8233 time: 0.0209s\n",
      "Epoch: 0163 loss_train: 0.5226 acc_train: 0.9143 loss_val: 0.7345 acc_val: 0.8200 time: 0.0215s\n",
      "Epoch: 0164 loss_train: 0.4722 acc_train: 0.9214 loss_val: 0.7330 acc_val: 0.8167 time: 0.0223s\n",
      "Epoch: 0165 loss_train: 0.4759 acc_train: 0.9357 loss_val: 0.7317 acc_val: 0.8100 time: 0.0198s\n",
      "Epoch: 0166 loss_train: 0.4844 acc_train: 0.9429 loss_val: 0.7308 acc_val: 0.8100 time: 0.0222s\n",
      "Epoch: 0167 loss_train: 0.4597 acc_train: 0.9429 loss_val: 0.7286 acc_val: 0.8100 time: 0.0202s\n",
      "Epoch: 0168 loss_train: 0.4806 acc_train: 0.9357 loss_val: 0.7256 acc_val: 0.8100 time: 0.0225s\n",
      "Epoch: 0169 loss_train: 0.4581 acc_train: 0.9286 loss_val: 0.7234 acc_val: 0.8100 time: 0.0212s\n",
      "Epoch: 0170 loss_train: 0.4431 acc_train: 0.9143 loss_val: 0.7217 acc_val: 0.8100 time: 0.0237s\n",
      "Epoch: 0171 loss_train: 0.4704 acc_train: 0.9429 loss_val: 0.7202 acc_val: 0.8067 time: 0.0233s\n",
      "Epoch: 0172 loss_train: 0.4511 acc_train: 0.9214 loss_val: 0.7191 acc_val: 0.8100 time: 0.0227s\n",
      "Epoch: 0173 loss_train: 0.4916 acc_train: 0.9357 loss_val: 0.7182 acc_val: 0.8100 time: 0.0252s\n",
      "Epoch: 0174 loss_train: 0.4297 acc_train: 0.9500 loss_val: 0.7175 acc_val: 0.8100 time: 0.0251s\n",
      "Epoch: 0175 loss_train: 0.4466 acc_train: 0.9214 loss_val: 0.7166 acc_val: 0.8100 time: 0.0239s\n",
      "Epoch: 0176 loss_train: 0.4281 acc_train: 0.9429 loss_val: 0.7167 acc_val: 0.8167 time: 0.0236s\n",
      "Epoch: 0177 loss_train: 0.4489 acc_train: 0.9429 loss_val: 0.7168 acc_val: 0.8133 time: 0.0240s\n",
      "Epoch: 0178 loss_train: 0.4241 acc_train: 0.9500 loss_val: 0.7164 acc_val: 0.8167 time: 0.0255s\n",
      "Epoch: 0179 loss_train: 0.4863 acc_train: 0.9143 loss_val: 0.7125 acc_val: 0.8133 time: 0.0226s\n",
      "Epoch: 0180 loss_train: 0.4010 acc_train: 0.9357 loss_val: 0.7078 acc_val: 0.8167 time: 0.0240s\n",
      "Epoch: 0181 loss_train: 0.4217 acc_train: 0.9286 loss_val: 0.7048 acc_val: 0.8100 time: 0.0254s\n",
      "Epoch: 0182 loss_train: 0.4466 acc_train: 0.9143 loss_val: 0.7021 acc_val: 0.8100 time: 0.0217s\n",
      "Epoch: 0183 loss_train: 0.4412 acc_train: 0.9429 loss_val: 0.7007 acc_val: 0.8067 time: 0.0220s\n",
      "Epoch: 0184 loss_train: 0.4375 acc_train: 0.9429 loss_val: 0.6994 acc_val: 0.8067 time: 0.0215s\n",
      "Epoch: 0185 loss_train: 0.4428 acc_train: 0.9429 loss_val: 0.6988 acc_val: 0.8067 time: 0.0221s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0186 loss_train: 0.4254 acc_train: 0.9429 loss_val: 0.6985 acc_val: 0.8067 time: 0.0195s\n",
      "Epoch: 0187 loss_train: 0.4422 acc_train: 0.9214 loss_val: 0.6971 acc_val: 0.8100 time: 0.0226s\n",
      "Epoch: 0188 loss_train: 0.4553 acc_train: 0.9429 loss_val: 0.6976 acc_val: 0.8100 time: 0.0218s\n",
      "Epoch: 0189 loss_train: 0.4115 acc_train: 0.9286 loss_val: 0.6977 acc_val: 0.8067 time: 0.0208s\n",
      "Epoch: 0190 loss_train: 0.3801 acc_train: 0.9357 loss_val: 0.6986 acc_val: 0.8067 time: 0.0192s\n",
      "Epoch: 0191 loss_train: 0.4230 acc_train: 0.9286 loss_val: 0.6985 acc_val: 0.8067 time: 0.0190s\n",
      "Epoch: 0192 loss_train: 0.4388 acc_train: 0.9143 loss_val: 0.6984 acc_val: 0.8100 time: 0.0180s\n",
      "Epoch: 0193 loss_train: 0.4165 acc_train: 0.9286 loss_val: 0.6962 acc_val: 0.8133 time: 0.0211s\n",
      "Epoch: 0194 loss_train: 0.3903 acc_train: 0.9214 loss_val: 0.6929 acc_val: 0.8167 time: 0.0205s\n",
      "Epoch: 0195 loss_train: 0.4177 acc_train: 0.9357 loss_val: 0.6900 acc_val: 0.8167 time: 0.0198s\n",
      "Epoch: 0196 loss_train: 0.3756 acc_train: 0.9714 loss_val: 0.6879 acc_val: 0.8167 time: 0.0218s\n",
      "Epoch: 0197 loss_train: 0.4144 acc_train: 0.9643 loss_val: 0.6859 acc_val: 0.8133 time: 0.0227s\n",
      "Epoch: 0198 loss_train: 0.4333 acc_train: 0.9000 loss_val: 0.6841 acc_val: 0.8167 time: 0.0209s\n",
      "Epoch: 0199 loss_train: 0.4333 acc_train: 0.8929 loss_val: 0.6830 acc_val: 0.8167 time: 0.0224s\n",
      "Epoch: 0200 loss_train: 0.4000 acc_train: 0.9714 loss_val: 0.6825 acc_val: 0.8200 time: 0.0185s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 4.5894s\n",
      "Test set results: loss= 0.7221 accuracy= 0.8260\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T08:34:22.478424Z",
     "start_time": "2019-01-20T08:34:22.473867Z"
    }
   },
   "outputs": [],
   "source": [
    "import pygat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T08:34:16.962167Z",
     "start_time": "2019-01-20T08:34:16.849664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT.ipynb  cnn.png    cnn2.png   gcn.png    gcn2.png   \u001b[34mpygat\u001b[m\u001b[m/     walk.png\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T03:57:57.011392Z",
     "start_time": "2019-01-20T03:57:56.897377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mREADME\u001b[m\u001b[m*       \u001b[31mcora.cites\u001b[m\u001b[m*   \u001b[31mcora.content\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "ls data/cora/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T04:02:00.465681Z",
     "start_time": "2019-01-21T04:02:00.456105Z"
    }
   },
   "source": [
    "# Graph Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a graph of **n** nodes\n",
    "- node features: $(\\vec{h}_1, \\vec{h}_2, \\dots, \\vec{h}_n)$\n",
    "- adjacency matrix $A$\n",
    "\n",
    "To get a higher-level representation of node features\n",
    "\n",
    "$\\vec{g}_i = {\\bf W}\\vec{h}_i$\n",
    "\n",
    "http://petar-v.com/GAT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a graph convolutional operator as an aggregation of features across neighbourhoods. \n",
    "- The output features of node $i$  \n",
    "\n",
    "$$\\vec{h}'_i = \\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\vec{g}_j\\right)$$\n",
    "\n",
    "- $\\sigma$ is an activation function, e.g., softmax\n",
    "- $a_{ij}$ specifies the weighting factor (importance) of node j’s features to node i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $a_{ij}$ be implicitly defined\n",
    "- employing **self-attention** over the node features to do so, $a : \\mathbb{R}^N \\times \\mathbb{R}^N \\rightarrow \\mathbb{R}$\n",
    "\n",
    "$$e_{ij} = a(\\vec{h}_i, \\vec{h}_j)$$\n",
    "\n",
    "- Normalised using the softmax function:\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k\\in\\mathcal{N}_i}\\exp(e_{ik})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularisation**\n",
    "- To stabilise the learning process of self-attention\n",
    "$$\\vec{h}'_i = {\\LARGE \\|}_{k=1}^K \\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}^k{\\bf W}^k\\vec{h}_j\\right)$$\n",
    "\n",
    "where \n",
    "- ∥ represents concatenation\n",
    "- $K$ denotes the number of independent attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T04:59:20.146704Z",
     "start_time": "2019-01-21T04:59:20.142677Z"
    }
   },
   "source": [
    "<img src = './attention.png' width = '700'>\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every neighbour **i** of node sends its own vector of attentional coefficients, \n",
    "    - $\\vec{\\alpha}_{1i}$ one per each attention head $\\alpha_{1i}^k$\n",
    "- To compute **K** separate linear combinations of neighbours’ features  $\\vec{h}_i$\n",
    "- Aggregate (typically by concatenation or averaging) to obtain the next-level features of node $\\vec{h}’_1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T04:58:39.956209Z",
     "start_time": "2019-01-21T04:58:39.833556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mLICENSE\u001b[m\u001b[m*            \u001b[34mdata\u001b[m\u001b[m/               \u001b[34moutput\u001b[m\u001b[m/             \u001b[31mutils.py\u001b[m\u001b[m*\r\n",
      "\u001b[31mREADME.md\u001b[m\u001b[m*          \u001b[31mlayers.py\u001b[m\u001b[m*          \u001b[31mrequirements.txt\u001b[m\u001b[m*   \u001b[31mvisualize_graph.py\u001b[m\u001b[m*\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m/        \u001b[31mmodels.py\u001b[m\u001b[m*          \u001b[31mtrain.py\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T04:06:19.714345Z",
     "start_time": "2019-01-21T04:06:19.709498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/datalab/github/deeplearning-winter2019/pyGAT-master/pygat\n"
     ]
    }
   ],
   "source": [
    "cd pygat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T04:05:25.146743Z",
     "start_time": "2019-01-21T04:05:25.138667Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T04:06:35.831789Z",
     "start_time": "2019-01-21T04:06:35.819825Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import load_data, accuracy\n",
    "from models import GAT, SpGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T06:58:20.193405Z",
     "start_time": "2019-01-21T06:58:20.175168Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda=True\n",
    "        self.cuda=False\n",
    "        self.fastmode=False\n",
    "        self.sparse=False\n",
    "        self.seed=72\n",
    "        self.epochs=2\n",
    "        self.lr=5e-3\n",
    "        self.weight_decay=5e-4\n",
    "        self.hidden=8\n",
    "        self.nb_heads=8\n",
    "        self.dropout=0.6\n",
    "        self.alpha=0.2\n",
    "        self.patience=100\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T04:07:57.679685Z",
     "start_time": "2019-01-21T04:07:51.409477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T04:12:29.546834Z",
     "start_time": "2019-01-21T04:12:29.533250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "if args.sparse:\n",
    "    model = SpGAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "else:\n",
    "    model = GAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:26:49.227342Z",
     "start_time": "2019-01-21T07:26:49.219931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(labels.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T04:08:36.099982Z",
     "start_time": "2019-01-21T04:08:36.091954Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T06:59:43.796990Z",
     "start_time": "2019-01-21T06:58:30.258347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.6168 acc_train: 0.5071 loss_val: 1.6278 acc_val: 0.5767 time: 31.6586s\n",
      "Epoch: 0002 loss_train: 1.5773 acc_train: 0.5714 loss_val: 1.6154 acc_val: 0.5767 time: 30.0393s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 62.3707s\n",
      "Loading 1th epoch\n",
      "Test set results: loss= 1.6820 accuracy= 0.4440\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.item()\n",
    "\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "\n",
    "# Testing\n",
    "compute_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T06:34:10.287300Z",
     "start_time": "2019-01-21T06:34:10.282858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9512795209884644"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_train.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
